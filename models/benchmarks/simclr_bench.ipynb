{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from simclr import SimCLR\n",
    "from simclr.modules import get_resnet, NT_Xent\n",
    "from simclr.modules.transformations import TransformsSimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Hyperparameters, can change for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'image_size': 224,\n",
    "    'resnet': 'resnet18',\n",
    "    'temperature': 0.5,\n",
    "    'weight_decay': 5e-5,\n",
    "    'learning_rate': 1e-5,\n",
    "    'dataset_dir': '../../data/'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Loading/Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# args['dataset'] = 'STL10'\n",
    "# dataset = torchvision.datasets.STL10(args['dataset_dir'], split='unlabeled', download=True, transform=TransformsSimCLR(size=args['image_size']))\n",
    "args['dataset'] = 'CIFAR100'\n",
    "dataset = torchvision.datasets.CIFAR100(args['dataset_dir'], download=True, transform=TransformsSimCLR(size=args['image_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jared\\Documents\\CS7643\\project\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jared\\Documents\\CS7643\\project\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "encoder = get_resnet(args['resnet'])\n",
    "model = SimCLR(encoder, 64, encoder.fc.in_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), args['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = NT_Xent(args['batch_size'], args['temperature'], world_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, training_loader):\n",
    "    total_loss = 0\n",
    "    for step, ((xi, xj), _) in enumerate(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "        #need to make the tensor the right type or it will fail\n",
    "        xi = xi.to(device)\n",
    "        xj = xj.to(device)\n",
    "        #xi, xj are the two correlated augmented examples\n",
    "        #first two are the represenatations, and try to maximize z agreements\n",
    "        _, _, zi, zj = model(xi, xj)\n",
    "        if zi.size()[0] == 64:\n",
    "            loss = loss_func(zi, zj)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if step % 130 == 0:\n",
    "                print(f'Step {step} Loss: {loss.item()}')\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Loss: 4.815101623535156\n",
      "Step 130 Loss: 4.661430358886719\n",
      "Step 260 Loss: 4.452215671539307\n",
      "Step 390 Loss: 4.550997734069824\n",
      "Step 520 Loss: 4.545881271362305\n",
      "Step 650 Loss: 4.3085126876831055\n",
      "Step 780 Loss: 4.443071365356445\n",
      "Epoch 0 Loss: 3480.9206972122192\n",
      "Step 0 Loss: 4.418758392333984\n",
      "Step 130 Loss: 4.337266445159912\n",
      "Step 260 Loss: 4.290072917938232\n",
      "Step 390 Loss: 4.3134660720825195\n",
      "Step 520 Loss: 4.418010711669922\n",
      "Step 650 Loss: 4.3648247718811035\n",
      "Step 780 Loss: 4.3372931480407715\n",
      "Epoch 1 Loss: 3355.4564394950867\n",
      "Step 0 Loss: 4.23141622543335\n",
      "Step 130 Loss: 4.1005425453186035\n",
      "Step 260 Loss: 4.030501365661621\n",
      "Step 390 Loss: 4.066650867462158\n",
      "Step 520 Loss: 4.0562920570373535\n",
      "Step 650 Loss: 4.0688371658325195\n",
      "Step 780 Loss: 4.1395487785339355\n",
      "Epoch 2 Loss: 3290.510726213455\n",
      "Step 0 Loss: 4.162351608276367\n",
      "Step 130 Loss: 4.160772800445557\n",
      "Step 260 Loss: 4.085745811462402\n",
      "Step 390 Loss: 4.122556209564209\n",
      "Step 520 Loss: 4.175682067871094\n",
      "Step 650 Loss: 4.183428764343262\n",
      "Step 780 Loss: 4.259134292602539\n",
      "Epoch 3 Loss: 3245.786762237549\n",
      "Step 0 Loss: 4.0044779777526855\n",
      "Step 130 Loss: 4.227158546447754\n",
      "Step 260 Loss: 4.0198893547058105\n",
      "Step 390 Loss: 3.965669631958008\n",
      "Step 520 Loss: 4.140777111053467\n",
      "Step 650 Loss: 4.027833461761475\n",
      "Step 780 Loss: 4.139158248901367\n",
      "Epoch 4 Loss: 3206.3482472896576\n",
      "Step 0 Loss: 3.996860980987549\n",
      "Step 130 Loss: 3.987107038497925\n",
      "Step 260 Loss: 4.055109024047852\n",
      "Step 390 Loss: 4.069189548492432\n",
      "Step 520 Loss: 4.204506874084473\n",
      "Step 650 Loss: 3.9686191082000732\n",
      "Step 780 Loss: 4.063762187957764\n",
      "Epoch 5 Loss: 3175.8272132873535\n",
      "Step 0 Loss: 4.150306701660156\n",
      "Step 130 Loss: 4.041574478149414\n",
      "Step 260 Loss: 4.142607688903809\n",
      "Step 390 Loss: 4.099241256713867\n",
      "Step 520 Loss: 3.9691555500030518\n",
      "Step 650 Loss: 4.10942268371582\n",
      "Step 780 Loss: 4.072515487670898\n",
      "Epoch 6 Loss: 3149.033509492874\n",
      "Step 0 Loss: 4.030865669250488\n",
      "Step 130 Loss: 4.0142669677734375\n",
      "Step 260 Loss: 3.9516797065734863\n",
      "Step 390 Loss: 3.984863758087158\n",
      "Step 520 Loss: 3.94187068939209\n",
      "Step 650 Loss: 3.9276108741760254\n",
      "Step 780 Loss: 4.053538799285889\n",
      "Epoch 7 Loss: 3133.273044347763\n",
      "Step 0 Loss: 4.001529693603516\n",
      "Step 130 Loss: 4.166414260864258\n",
      "Step 260 Loss: 4.064477920532227\n",
      "Step 390 Loss: 4.051475524902344\n",
      "Step 520 Loss: 3.981328010559082\n",
      "Step 650 Loss: 4.058969020843506\n",
      "Step 780 Loss: 3.9207565784454346\n",
      "Epoch 8 Loss: 3119.1906661987305\n",
      "Step 0 Loss: 4.123608589172363\n",
      "Step 130 Loss: 3.881279706954956\n",
      "Step 260 Loss: 3.887559652328491\n",
      "Step 390 Loss: 3.918944835662842\n",
      "Step 520 Loss: 4.147705554962158\n",
      "Step 650 Loss: 4.050832748413086\n",
      "Step 780 Loss: 3.906543254852295\n",
      "Epoch 9 Loss: 3104.9833676815033\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    epoch_loss = train(model, optimizer, loss_func, training_loader)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simclr.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (encoder): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=64, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('simclr.tar'))\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained model from https://github.com/Spijkervet/SimCLR/releases/download/1.2/ (much better given 100 epochs of training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jared\\Documents\\CS7643\\project\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jared\\Documents\\CS7643\\project\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (encoder): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=64, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = get_resnet('resnet50', pretrained=False)\n",
    "model2 = SimCLR(resnet, 64, resnet.fc.in_features)\n",
    "model2.load_state_dict(torch.load('checkpoint_100.tar'), strict=False)\n",
    "model2.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data setup for evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_dataset = torchvision.datasets.CIFAR100(args['dataset_dir'], download=True, transform=TransformsSimCLR(size=args['image_size']).test_transform, train=True)\n",
    "N = len(training_dataset)\n",
    "dataset_pct = 0.5 #change this for percent of data to train on for efficient training metric\n",
    "num_train_samples = int(N * dataset_pct)\n",
    "dataset_indices = np.random.choice(N, num_train_samples, replace=False)\n",
    "dataset_subset = torch.utils.data.Subset(training_dataset, dataset_indices)\n",
    "\n",
    "# split into train/val\n",
    "N_subset = len(dataset_subset)\n",
    "V = int(num_train_samples * 0.2) # 20% validation\n",
    "dataset_train, dataset_val = torch.utils.data.random_split(dataset_subset, [N_subset - V, V])\n",
    "test_dataset = torchvision.datasets.CIFAR100(args['dataset_dir'], download=True, transform=TransformsSimCLR(size=args['image_size']).test_transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader= torch.utils.data.DataLoader(dataset_train, batch_size=args['batch_size'], shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset_val, batch_size=args['batch_size'], shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Logistic Regression for evaluation (we may do something differnt to better match the CPC framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticModel(torch.nn.Module):\n",
    "    def __init__(self, features, classes):\n",
    "        super(LogisticModel, self).__init__()\n",
    "        self.model = torch.nn.Linear(features, classes)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the number of features to 10 or 100 depending on CIFAR\n",
    "logistic_model = LogisticModel(model2.n_features, 100).to(device) #model is the simclr as defined above\n",
    "optimizer= torch.optim.Adam(logistic_model.parameters(), lr=1e-4)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder to label the data with their represenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(loader, simclr):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for _, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            xi, _, _, _ = simclr(x, x)\n",
    "        \n",
    "        xi = xi.detach()\n",
    "        features.extend(xi.cpu().detach().numpy())\n",
    "        labels.extend(y.numpy())\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "def populate_labels(simclr, training_loader, validation_loader, test_loader):\n",
    "    X_train, y_train = encoder(training_loader, simclr)\n",
    "    X_val, y_val = encoder(validation_loader, simclr)\n",
    "    X_test, y_test = encoder(test_loader, simclr)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaders(X_train, y_train, X_val, y_val, X_test, y_test, batch):\n",
    "    training = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    validation = torch.utils.data.TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "    testing = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "    training_loader_final = torch.utils.data.DataLoader(training, batch_size=batch, shuffle=False)\n",
    "    validation_loader_final = torch.utils.data.DataLoader(validation, batch_size=batch, shuffle=False)\n",
    "    test_loader_final = torch.utils.data.DataLoader(testing, batch_size=batch, shuffle=False)\n",
    "    return training_loader_final, validation_loader_final, test_loader_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train, X_val, y_val, X_test, y_test) = populate_labels(model2, training_loader, validation_loader, test_loader)\n",
    "training_loader_final, validation_loader_final, test_loader_final = loaders(X_train, y_train, X_val, y_val, X_test, y_test, args['batch_size'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on patched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(logistic, loss_func, optimizer, data):\n",
    "    overall_accuracy = 0\n",
    "    total_loss = 0\n",
    "    for _, (x, y) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        x =x.to(device)\n",
    "        y =y.to(device)\n",
    "\n",
    "        out = logistic(x)\n",
    "        loss = loss_func(out, y)\n",
    "\n",
    "        guess = out.argmax(1)\n",
    "        accuracy = (guess==y).sum().item() /  y.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        overall_accuracy += accuracy\n",
    "\n",
    "    return total_loss, overall_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(logistic, data):\n",
    "    overall_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (x, y) in enumerate(data):\n",
    "            x =x.to(device)\n",
    "            y =y.to(device)\n",
    "\n",
    "            out = logistic(x)\n",
    "            guess = out.argmax(1)\n",
    "            accuracy = (guess==y).sum().item() /  y.size(0)\n",
    "            overall_accuracy += accuracy\n",
    "\n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(logistic_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 3.733852380380844 Accuracy: 0.16164137380191693\n",
      "Epoch 0 Validation Accuracy: 0.18947784810126583\n",
      "Epoch 1 Loss: 3.3106722626061487 Accuracy: 0.2248402555910543\n",
      "Epoch 1 Validation Accuracy: 0.2142009493670886\n",
      "Epoch 2 Loss: 3.1472154600551714 Accuracy: 0.25324480830670926\n",
      "Epoch 2 Validation Accuracy: 0.22626582278481014\n",
      "Epoch 3 Loss: 3.0287524366531127 Accuracy: 0.2744608626198083\n",
      "Epoch 3 Validation Accuracy: 0.23516613924050633\n",
      "Epoch 4 Loss: 2.934362782457004 Accuracy: 0.29018570287539935\n",
      "Epoch 4 Validation Accuracy: 0.24347310126582278\n",
      "Epoch 5 Loss: 2.855410454753108 Accuracy: 0.30501198083067094\n",
      "Epoch 5 Validation Accuracy: 0.25197784810126583\n",
      "Epoch 6 Loss: 2.787312414699469 Accuracy: 0.3176916932907348\n",
      "Epoch 6 Validation Accuracy: 0.2614715189873418\n",
      "Epoch 7 Loss: 2.727297328912412 Accuracy: 0.32992212460063897\n",
      "Epoch 7 Validation Accuracy: 0.26542721518987344\n",
      "Epoch 8 Loss: 2.673549623154223 Accuracy: 0.3404552715654952\n",
      "Epoch 8 Validation Accuracy: 0.270371835443038\n",
      "Epoch 9 Loss: 2.624811604761849 Accuracy: 0.3501397763578275\n",
      "Epoch 9 Validation Accuracy: 0.27610759493670883\n",
      "Epoch 10 Loss: 2.5801714597799528 Accuracy: 0.36112220447284343\n",
      "Epoch 10 Validation Accuracy: 0.27986550632911394\n",
      "Epoch 11 Loss: 2.538944077187072 Accuracy: 0.3702575878594249\n",
      "Epoch 11 Validation Accuracy: 0.28283227848101267\n",
      "Epoch 12 Loss: 2.5006025225971453 Accuracy: 0.3780451277955272\n",
      "Epoch 12 Validation Accuracy: 0.28619462025316456\n",
      "Epoch 13 Loss: 2.464733488643512 Accuracy: 0.38553314696485624\n",
      "Epoch 13 Validation Accuracy: 0.28955696202531644\n",
      "Epoch 14 Loss: 2.431005193402592 Accuracy: 0.3934205271565495\n",
      "Epoch 14 Validation Accuracy: 0.2915348101265823\n",
      "Epoch 15 Loss: 2.3991453076323 Accuracy: 0.4010583067092652\n",
      "Epoch 15 Validation Accuracy: 0.29291930379746833\n",
      "Epoch 16 Loss: 2.3689286644085525 Accuracy: 0.4069488817891374\n",
      "Epoch 16 Validation Accuracy: 0.29430379746835444\n",
      "Epoch 17 Loss: 2.3401696853363476 Accuracy: 0.41278953674121405\n",
      "Epoch 17 Validation Accuracy: 0.2939082278481013\n",
      "Epoch 18 Loss: 2.3127147039285485 Accuracy: 0.41902955271565495\n",
      "Epoch 18 Validation Accuracy: 0.2956882911392405\n",
      "Epoch 19 Loss: 2.286434519024322 Accuracy: 0.4240714856230032\n",
      "Epoch 19 Validation Accuracy: 0.29707278481012656\n",
      "Epoch 20 Loss: 2.261219288975286 Accuracy: 0.42931309904153353\n",
      "Epoch 20 Validation Accuracy: 0.29944620253164556\n",
      "Epoch 21 Loss: 2.2369744724358993 Accuracy: 0.4343550319488818\n",
      "Epoch 21 Validation Accuracy: 0.30063291139240506\n",
      "Epoch 22 Loss: 2.213618026373866 Accuracy: 0.43849840255591055\n",
      "Epoch 22 Validation Accuracy: 0.3010284810126582\n",
      "Epoch 23 Loss: 2.191078359707476 Accuracy: 0.44314097444089456\n",
      "Epoch 23 Validation Accuracy: 0.3022151898734177\n",
      "Epoch 24 Loss: 2.169292482705162 Accuracy: 0.4488817891373802\n",
      "Epoch 24 Validation Accuracy: 0.3032041139240506\n",
      "Epoch 25 Loss: 2.14820467625944 Accuracy: 0.45307507987220447\n",
      "Epoch 25 Validation Accuracy: 0.3035996835443038\n",
      "Epoch 26 Loss: 2.1277654190033006 Accuracy: 0.4571186102236422\n",
      "Epoch 26 Validation Accuracy: 0.3037974683544304\n",
      "Epoch 27 Loss: 2.1079305814097102 Accuracy: 0.4615615015974441\n",
      "Epoch 27 Validation Accuracy: 0.3035996835443038\n",
      "Epoch 28 Loss: 2.0886605071564452 Accuracy: 0.46630391373801916\n",
      "Epoch 28 Validation Accuracy: 0.30636867088607594\n",
      "Epoch 29 Loss: 2.0699194235542713 Accuracy: 0.47064696485623003\n",
      "Epoch 29 Validation Accuracy: 0.307753164556962\n",
      "Epoch 30 Loss: 2.051675083538214 Accuracy: 0.4741912939297125\n",
      "Epoch 30 Validation Accuracy: 0.3091376582278481\n",
      "Epoch 31 Loss: 2.0338980603141907 Accuracy: 0.4779852236421725\n",
      "Epoch 31 Validation Accuracy: 0.310126582278481\n",
      "Epoch 32 Loss: 2.0165616033937983 Accuracy: 0.4820287539936102\n",
      "Epoch 32 Validation Accuracy: 0.31091772151898733\n",
      "Epoch 33 Loss: 1.9996412958199985 Accuracy: 0.48647164536741216\n",
      "Epoch 33 Validation Accuracy: 0.3113132911392405\n",
      "Epoch 34 Loss: 1.9831146512168665 Accuracy: 0.49016573482428116\n",
      "Epoch 34 Validation Accuracy: 0.31329113924050633\n",
      "Epoch 35 Loss: 1.9669610589457016 Accuracy: 0.4932108626198083\n",
      "Epoch 35 Validation Accuracy: 0.31269778481012656\n",
      "Epoch 36 Loss: 1.951161458088567 Accuracy: 0.49640575079872207\n",
      "Epoch 36 Validation Accuracy: 0.3142800632911392\n",
      "Epoch 37 Loss: 1.9356982978388144 Accuracy: 0.5002496006389776\n",
      "Epoch 37 Validation Accuracy: 0.31507120253164556\n",
      "Epoch 38 Loss: 1.9205552004396724 Accuracy: 0.5041433706070287\n",
      "Epoch 38 Validation Accuracy: 0.31645569620253167\n",
      "Epoch 39 Loss: 1.9057171782746483 Accuracy: 0.5078374600638977\n",
      "Epoch 39 Validation Accuracy: 0.31764240506329117\n",
      "Epoch 40 Loss: 1.8911701494131605 Accuracy: 0.5116813099041534\n",
      "Epoch 40 Validation Accuracy: 0.31843354430379744\n",
      "Epoch 41 Loss: 1.8769009913118502 Accuracy: 0.514676517571885\n",
      "Epoch 41 Validation Accuracy: 0.3190268987341772\n",
      "Epoch 42 Loss: 1.8628975387198476 Accuracy: 0.5187200479233227\n",
      "Epoch 42 Validation Accuracy: 0.31981803797468356\n",
      "Epoch 43 Loss: 1.8491483877261226 Accuracy: 0.5218650159744409\n",
      "Epoch 43 Validation Accuracy: 0.3206091772151899\n",
      "Epoch 44 Loss: 1.8356428820485124 Accuracy: 0.5245607028753994\n",
      "Epoch 44 Validation Accuracy: 0.32080696202531644\n",
      "Epoch 45 Loss: 1.8223710155334716 Accuracy: 0.5279552715654952\n",
      "Epoch 45 Validation Accuracy: 0.3214003164556962\n",
      "Epoch 46 Loss: 1.809323468909096 Accuracy: 0.5311002396166135\n",
      "Epoch 46 Validation Accuracy: 0.3217958860759494\n",
      "Epoch 47 Loss: 1.7964914637251783 Accuracy: 0.5336461661341853\n",
      "Epoch 47 Validation Accuracy: 0.32377373417721517\n",
      "Epoch 48 Loss: 1.7838668297654905 Accuracy: 0.5369408945686901\n",
      "Epoch 48 Validation Accuracy: 0.3239715189873418\n",
      "Epoch 49 Loss: 1.7714416759844405 Accuracy: 0.5399361022364217\n",
      "Epoch 49 Validation Accuracy: 0.3247626582278481\n"
     ]
    }
   ],
   "source": [
    "random_seed = 15009\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "for epoch in range(50):\n",
    "    epoch_loss, epoch_accuracy = train(logistic_model, loss_func, optimizer, training_loader_final)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss/len(training_loader_final)} Accuracy: {epoch_accuracy/len(training_loader_final)}\")\n",
    "    validation_accuracy = validation(logistic_model, validation_loader_final)\n",
    "    print(f\"Epoch {epoch} Validation Accuracy: {validation_accuracy/len(validation_loader_final)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_eval(logistic, data):\n",
    "    overall_accuracy = 0\n",
    "    total_loss = 0\n",
    "    logistic.eval()\n",
    "    for _, (x,y) in enumerate(data):\n",
    "        x= x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out = logistic(x)\n",
    "        loss = loss_func(out, y)\n",
    "        guess = out.argmax(1)\n",
    "        accuracy = (guess==y).sum().item() /  y.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        overall_accuracy += accuracy\n",
    "    return total_loss, overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.245242929762336, Accuracy: 0.4211783439490446\n"
     ]
    }
   ],
   "source": [
    "random_seed = 15009\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "loss, accuracy = final_eval(logistic_model, test_loader_final)\n",
    "print( f'Loss: {loss/len(test_loader_final)}, Accuracy: {accuracy/len(test_loader_final)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
